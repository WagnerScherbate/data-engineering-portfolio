# data-engineering-portfolio
# ğŸ—ï¸ Portfolio de Engenharia de Dados - Arquitetura MedalhÃ£o

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)
![Kafka](https://img.shields.io/badge/Apache-Kafka-black.svg)
![Spark](https://img.shields.io/badge/Apache-Spark-orange.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![DBT](https://img.shields.io/badge/DBT-Core-orange.svg)

## ğŸ“– Sobre o Projeto

Este projeto demonstra uma implementaÃ§Ã£o completa de uma arquitetura de dados moderna utilizando o padrÃ£o **MedalhÃ£o** (Bronze, Silver, Gold), com ingestÃ£o batch e streaming de dados fictÃ­cios gerados pela biblioteca Faker.

### ğŸ¯ Objetivo

Simular um ambiente de engenharia de dados end-to-end para um e-commerce fictÃ­cio, abrangendo desde a geraÃ§Ã£o de dados atÃ© anÃ¡lises avanÃ§adas.

## ğŸ›ï¸ Arquitetura MedalhÃ£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CAMADA GOLD                              â”‚
â”‚  (Dados Agregados e Otimizados para AnÃ¡lise)               â”‚
â”‚  â€¢ KPIs e MÃ©tricas de NegÃ³cio                               â”‚
â”‚  â€¢ Modelagem Dimensional (Star Schema)                      â”‚
â”‚  â€¢ PostgreSQL + Databricks                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚
                    DBT + Spark Processing
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CAMADA SILVER                            â”‚
â”‚  (Dados Limpos, Validados e Transformados)                  â”‚
â”‚  â€¢ DeduplicaÃ§Ã£o e Limpeza                                   â”‚
â”‚  â€¢ PadronizaÃ§Ã£o de Formatos                                 â”‚
â”‚  â€¢ Parquet/Delta Format                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚
                   Python + PySpark ETL
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CAMADA BRONZE                            â”‚
â”‚  (Dados Brutos - Raw Data)                                  â”‚
â”‚  â€¢ IngestÃ£o Batch (Python)                                  â”‚
â”‚  â€¢ IngestÃ£o Streaming (Kafka)                               â”‚
â”‚  â€¢ Parquet Format                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                       â”‚
         Batch Ingestion        Streaming Ingestion
         (Python Script)         (Kafka Producer)
                â”‚                       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Faker Library â”‚
                    â”‚ (Dados Ficticios)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ Estrutura do Projeto

```
data-engineering-portfolio/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ faker_generator.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_ingestion.py
â”‚   â”‚   â””â”€â”€ streaming_producer.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚   â””â”€â”€ silver_to_gold_spark.py
â”‚   â””â”€â”€ consumers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ streaming_consumer.py
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”œâ”€â”€ silver/
â”‚       â””â”€â”€ gold/
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ queries_analytics.sql
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ databricks_analysis.ipynb
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ spark/
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ start_batch.sh
    â”œâ”€â”€ start_streaming.sh
    â””â”€â”€ run_dbt.sh
```

## ğŸš€ Tecnologias Utilizadas

- **Python 3.10+**: Linguagem principal
- **Apache Kafka**: Streaming de dados em tempo real
- **Apache Spark**: Processamento distribuÃ­do
- **PostgreSQL**: Data Warehouse
- **DBT Core**: TransformaÃ§Ãµes SQL e testes
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o
- **Faker**: GeraÃ§Ã£o de dados fictÃ­cios
- **Pandas & PyArrow**: ManipulaÃ§Ã£o de dados
- **PgAdmin**: Interface web para PostgreSQL

## ğŸ“Š Datasets Gerados

O projeto simula um e-commerce com os seguintes datasets:

1. **Clientes** (10.000 registros)
   - ID, Nome, Email, Telefone, EndereÃ§o, Data de Cadastro

2. **Produtos** (1.000 registros)
   - ID, Nome, Categoria, PreÃ§o, Estoque, Fornecedor

3. **Pedidos** (50.000 registros)
   - ID, ID Cliente, Data, Valor Total, Status

4. **Itens de Pedido** (150.000 registros)
   - ID, ID Pedido, ID Produto, Quantidade, Valor UnitÃ¡rio

5. **Eventos de Website** (500.000+ registros - streaming)
   - ID Cliente, Timestamp, Tipo de Evento, PÃ¡gina, Session ID

## ğŸ”§ PrÃ©-requisitos

- Docker Desktop instalado
- Docker Compose
- Git
- 8GB+ de RAM disponÃ­vel
- 10GB+ de espaÃ§o em disco

## âš™ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/WagnerScherbate/data-engineering-portfolio.git
cd data-engineering-portfolio
```

### 2. Configure as variÃ¡veis de ambiente

```bash
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

### 3. Inicie os serviÃ§os com Docker

```bash
docker-compose up -d
```

Aguarde alguns minutos para todos os serviÃ§os iniciarem. VocÃª pode verificar o status:

```bash
docker-compose ps
```

### 4. Instale as dependÃªncias Python (opcional, para desenvolvimento local)

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## ğŸ® Como Usar

### 1ï¸âƒ£ IngestÃ£o Batch (Bronze Layer)

Execute o script de ingestÃ£o batch para gerar dados iniciais:

```bash
./scripts/start_batch.sh
```

Ou manualmente:

```bash
python src/ingestion/batch_ingestion.py
```

### 2ï¸âƒ£ IngestÃ£o Streaming (Bronze Layer)

Inicie o producer Kafka em um terminal:

```bash
python src/ingestion/streaming_producer.py
```

Inicie o consumer Kafka em outro terminal:

```bash
python src/consumers/streaming_consumer.py
```

### 3ï¸âƒ£ TransformaÃ§Ã£o Bronze â†’ Silver

```bash
python src/processing/bronze_to_silver.py
```

### 4ï¸âƒ£ TransformaÃ§Ã£o Silver â†’ Gold (Spark)

```bash
./scripts/start_spark_job.sh
```

Ou:

```bash
spark-submit src/processing/silver_to_gold_spark.py
```

### 5ï¸âƒ£ TransformaÃ§Ãµes DBT

```bash
cd dbt_project
dbt run
dbt test
dbt docs generate
dbt docs serve
```

### 6ï¸âƒ£ Acessar Interfaces Web

- **PgAdmin**: http://localhost:5050
  - Email: admin@admin.com
  - Senha: admin

- **Spark Master UI**: http://localhost:8080

- **Kafka UI** (opcional): http://localhost:9000

## ğŸ“ˆ AnÃ¡lises no Databricks Community

1. Acesse [Databricks Community Edition](https://community.cloud.databricks.com/)
2. Importe o notebook: `notebooks/databricks_analysis.ipynb`
3. FaÃ§a upload dos arquivos Parquet da camada Gold
4. Execute as anÃ¡lises e visualizaÃ§Ãµes

## ğŸ” Exemplos de Queries

### Top 10 Produtos Mais Vendidos

```sql
SELECT 
    p.nome_produto,
    SUM(ip.quantidade) as total_vendido,
    SUM(ip.quantidade * ip.valor_unitario) as receita_total
FROM gold.itens_pedido ip
JOIN gold.produtos p ON ip.id_produto = p.id_produto
GROUP BY p.nome_produto
ORDER BY total_vendido DESC
LIMIT 10;
```

### Receita Mensal

```sql
SELECT 
    DATE_TRUNC('month', data_pedido) as mes,
    COUNT(*) as total_pedidos,
    SUM(valor_total) as receita_mensal
FROM gold.pedidos
WHERE status = 'concluido'
GROUP BY mes
ORDER BY mes DESC;
```

## ğŸ“š Conceitos Demonstrados

- âœ… Arquitetura MedalhÃ£o (Bronze, Silver, Gold)
- âœ… IngestÃ£o Batch e Streaming
- âœ… ETL/ELT com Python e Spark
- âœ… Modelagem de Data Warehouse
- âœ… ContainerizaÃ§Ã£o com Docker
- âœ… Message Broker com Kafka
- âœ… TransformaÃ§Ãµes SQL com DBT
- âœ… Testes de Qualidade de Dados
- âœ… DocumentaÃ§Ã£o TÃ©cnica
- âœ… Processamento DistribuÃ­do

## ğŸ› Troubleshooting

### Problema: Kafka nÃ£o inicia

```bash
docker-compose down -v
docker-compose up -d zookeeper
# Aguarde 30 segundos
docker-compose up -d kafka
```

### Problema: Spark sem memÃ³ria

Ajuste no `docker-compose.yml`:

```yaml
environment:
  - SPARK_WORKER_MEMORY=2g
  - SPARK_DRIVER_MEMORY=1g
```

### Problema: PostgreSQL connection refused

Verifique se o container estÃ¡ rodando:

```bash
docker-compose logs postgres
```

## ğŸ“ PrÃ³ximos Passos

- [ ] Implementar Apache Airflow para orquestraÃ§Ã£o
- [ ] Adicionar Great Expectations para validaÃ§Ã£o de dados
- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar Monitoring com Prometheus + Grafana
- [ ] Implementar Data Lineage
- [ ] Adicionar testes de integraÃ§Ã£o
- [ ] Implementar CI/CD com GitHub Actions

## ğŸ‘¤ Autor

**Wagner Scherbate**

- GitHub: [@WagnerScherbate](https://github.com/WagnerScherbate)
- LinkedIn: [Seu LinkedIn]

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ Agradecimentos

- Biblioteca Faker pela geraÃ§Ã£o de dados
- Comunidade Apache por Kafka e Spark
- DBT Labs pelo DBT Core
- Databricks Community Edition

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela!